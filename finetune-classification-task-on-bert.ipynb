{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b59fb719-78bf-4dae-b645-446052f0fe53",
   "metadata": {},
   "source": [
    "gokartというパイプライン処理を実現するライブラリがある。\n",
    "\n",
    "gokartの解説はこちら\n",
    "\n",
    "\n",
    "gokartを用いて機械学習の処理をタスク単位で切り分けつつ、再利用性を高めたり、再実行の際にキャッシュを聞かせて素早く試行錯誤ができる状態を作りたい。\n",
    "\n",
    "今回は、既に実現しているtransformersのモデルとTraierクラスを用いたfinetuneの処理を、可能な範囲でgokartのタスク化していく。\n",
    "\n",
    "結果としては、学習結果のモデルの保存の都合を考えると、タスク化は部分的にとどめたほうがよいように感じた。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be20daac-4d44-4c43-8ebd-1b0bc3aa75f8",
   "metadata": {},
   "source": [
    "タスクの中身に関しては説明しないので、コードを読んでください。\n",
    "\n",
    "そもそもここでの作業は、./finetune-with-trtansformers.ipynbの内容を可能な範囲でタスク化したものなので、そちらのnotebookを見ていれば多分分かるはず"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ae8eb5ba-b8bc-409d-bee1-85c7cee8a94c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import luigi\n",
    "import gokart"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06a4415d-bdbf-44b6-9bae-093b5155f071",
   "metadata": {},
   "source": [
    "まずは、livedoorのコーパスのダウンロードとpandas dataframe化のタスクを用意したので以下のように使う。\n",
    "\n",
    "`data_task`はtaskのインスタンスであり、中身はまだ実行されてない。gokart.buildに渡すことで、タスクの中身が実行される。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3e09317c-9c94-48c5-a123-b615bf24a4bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tasks.data.livedoor import LivedoorCourpusData\n",
    "data_task = LivedoorCourpusData()\n",
    "\n",
    "df = gokart.build(data_task)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c368f313-636d-4179-ad3f-33b5be89b5d4",
   "metadata": {},
   "source": [
    "BERTのconfigとモデルのクラスを読み込むタスク"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2abc8e1e-9833-4c22-9270-204fc58a3f29",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bert import LoadBertConfig, LoadBertForSequenceClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8cca51dc-9a14-40bf-bcbc-1a4481c7d277",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "num_labels = len(df.category.unique())\n",
    "model_dir = '/mnt/d/yoheikikuta-bert-japanese/'\n",
    "base_ckpt = 'model.ckpt-1400000'    # 拡張子は含めない"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a052de40-dda6-446b-9690-b5bffc094337",
   "metadata": {},
   "source": [
    "num_labelsをdata_taskからbuildして取得しているが、これは後にConvertCategoryToLabelでnum_labelsを得るタスク定義したのでそっち使っても良かったかも"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b624e0cf-ee0f-4b44-86fb-b55fd08b9776",
   "metadata": {},
   "outputs": [],
   "source": [
    "config_task = LoadBertConfig(num_labels=num_labels)\n",
    "model_task = LoadBertForSequenceClassification(\n",
    "    config_task=config_task,\n",
    "    model_dir=model_dir,\n",
    "    base_ckpt=base_ckpt,\n",
    ")\n",
    "model = gokart.build(model_task)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "46c1719e-10b5-4871-ad5c-2c21dde4bfcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "graph.pbtxt\t\t\t\t\t    model.ckpt-1400000.meta\n",
      "jawiki-20181220-pages-articles-multistream.xml.bz2  wiki-ja.model\n",
      "model.ckpt-1400000.data-00000-of-00001\t\t    wiki-ja.vocab\n",
      "model.ckpt-1400000.index\n"
     ]
    }
   ],
   "source": [
    "! ls {model_dir}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "66b4e9e8-64f2-43f4-b85e-6e075dbd2f8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "DIR_BERT_KIKUTA = '/mnt/d/yoheikikuta-bert-japanese/'\n",
    "BASE_SPM = 'wiki-ja.model'\n",
    "BASE_VOCAB = 'wiki-ja.vocab'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b5633a4-81ca-4c8c-a54d-09233b3c1f1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ConvertCategoryToLabelは、data_taskを渡して、カテゴリ数を得るタスク\n",
    "\n",
    "CreateDatasetは、data_taskを渡して、transformersのDatasetに変換するタスク"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "680fdb95-09b8-4407-8042-018f44c871df",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tasks.data.convert import ConvertCategoryToLabel\n",
    "from tasks.data.dataset import CreateDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "50ae5dc0-e719-4c3d-8b17-73e4bf0dc4c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/miyamonz/ghq/github.com/miyamonz/bert-gokart/.venv/lib/python3.7/site-packages/luigi/parameter.py:279: UserWarning: Parameter \"required_columns\" with value \"('labels', 'text')\" is not of type string.\n",
      "  warnings.warn('Parameter \"{}\" with value \"{}\" is not of type string.'.format(param_name, param_value))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['labels', 'text'],\n",
       "        num_rows: 5525\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['labels', 'text'],\n",
       "        num_rows: 1842\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "task = ConvertCategoryToLabel(data_task=data_task, target_column='category')\n",
    "dataset_task = CreateDataset(data_task=task)\n",
    "gokart.build(dataset_task)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "73853a34-fa13-4209-bd20-41c8bcc61743",
   "metadata": {},
   "outputs": [],
   "source": [
    "DIR_BERT_KIKUTA = '/mnt/d/yoheikikuta-bert-japanese/'\n",
    "BASE_SPM = 'wiki-ja.model'\n",
    "BASE_VOCAB = 'wiki-ja.vocab'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36d308bc-8bc1-4d89-9928-483a61df03b1",
   "metadata": {},
   "source": [
    "tokenizerを返すタスクが以下。まだ別のファイルに切り出してないだけ\n",
    "\n",
    "（transformersに乗っかるなら、AutoTokenizer使って、タスクのパラメータとして文字列を受け取るといいかも？"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3164f1dc-c601-472f-a745-5abbda1201f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AlbertTokenizer\n",
    "\n",
    "class LoadSentencepieceTokenizer(gokart.TaskOnKart):\n",
    "    path = luigi.Parameter()\n",
    "    def run(self):\n",
    "        tokenizer = AlbertTokenizer(self.path)\n",
    "        self.dump(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "615977a7-1f99-45fb-ae97-af63a3732113",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_task=LoadSentencepieceTokenizer(path=DIR_BERT_KIKUTA + BASE_SPM)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6648ec6-674d-4b7a-8b68-0351f7b0dc7f",
   "metadata": {},
   "source": [
    "Datasetをtokenizeするタスク"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "35591764-7f8e-4c7b-af92-f385373507a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TokenizeDataset(gokart.TaskOnKart):\n",
    "    dataset_task = gokart.TaskInstanceParameter()\n",
    "    config_task = gokart.TaskInstanceParameter()\n",
    "    tokenizer_task = gokart.TaskInstanceParameter()\n",
    "\n",
    "    def run(self):\n",
    "        dataset = self.load(\"dataset_task\")\n",
    "        config = self.load(\"config_task\")\n",
    "        max_length = config.max_position_embeddings\n",
    "        tokenizer = self.load(\"tokenizer_task\")\n",
    "        \n",
    "        def tokenize(examples):\n",
    "            input_ids = tokenizer(examples[\"text\"], max_length=max_length, padding=\"max_length\",truncation=True,).input_ids\n",
    "            return {\n",
    "                \"input_ids\": input_ids,\n",
    "                \"labels\": examples['labels']\n",
    "            }\n",
    "\n",
    "        \n",
    "        tokenized = dataset.map(tokenize, batched=True, remove_columns=['text'])\n",
    "        tokenized.set_format(type='torch', columns=['input_ids', 'labels'])\n",
    "        self.dump(tokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0da07d4f-2355-4d98-b646-3a85f6d28076",
   "metadata": {},
   "outputs": [],
   "source": [
    "task = TokenizeDataset(dataset_task=dataset_task, config_task=config_task, tokenizer_task=tokenizer_task)\n",
    "tokenized_dataset = gokart.build(task)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a58d2b5a-b0a0-4043-a74b-db981c81a356",
   "metadata": {},
   "source": [
    "- データの取得\n",
    "- モデルの設定\n",
    "- モデル本体\n",
    "- Dataset\n",
    "- トークナイザ\n",
    "- Datasetのトークナイズ\n",
    "\n",
    "\n",
    "\n",
    "といった作業が、タスク単位で切り出された。異なる事前学習モデルを使うときは、タスク単位でモデルやトークナイザーを切り替えれば良い。\n",
    "\n",
    "今回は処理のながれをnotebook上でそのまま書いたが、タスクの読み込みとインスタンス化、パラメータとしての受け渡しを一つの関数としてまとめつつ、適切な抽象度でパラメータやタスクの切り替えをできるような形にすると良いと思われる\n",
    "\n",
    "これは、gokartを使わずにnotebook直書きの頃よりも、全体の見通しがしやすく、コードの再利用がしやすいように思える。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05fe1502-d50e-45a2-98f0-401c42147f84",
   "metadata": {},
   "source": [
    "## 学習\n",
    "\n",
    "できれば学習もタスク化したかったが、transformersのモデルの出力はフォルダ単位であり、これがgokartとは相性が悪いので諦めた。\n",
    "modelをそのままpklとしてdumpしちゃえばいいのはそれはそうであるが、なんか気持ち悪いので一旦やめておく。\n",
    "pklで気にせず保存して、gokart.buildで読み込みつつsave_pretrainedすればいつでもtransformersの形で保存しなおせはするのだが、ディスクに同じ内容のモデルがpklとpytorchのcheckpointの２種類で保存されることになり、容量を食いすぎる\n",
    "\n",
    "gokartで独自のフォルダをいい感じにdumpする方法は、ソースを見た感じ自分でも作れそうだったが、ちょっとだるいので一旦飛ばす。\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "学習に必要なdatasetを得るところまでがタスク化されてるだけでも、中間ファイルのキャッシュが効いて再実行が楽にできるので、部分的な導入も試行錯誤の速さには貢献している\n",
    "\n",
    "特定のパラメータで、どういうモデルが出力されたのか、というところはnotebook上で手書きで我慢する。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e9bf01d8-0c17-4555-b975-093f1f87a2d1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "# モデルの準備\n",
    "\n",
    "# Trainerのパラメータの準備\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results2',          # 出力フォルダ\n",
    "    num_train_epochs=10,              # エポック数\n",
    "    per_device_train_batch_size=4,  # 訓練のバッチサイズ\n",
    "    per_device_eval_batch_size=4,   # 評価のバッチサイズ\n",
    "    warmup_ratio=0.1,\n",
    "    weight_decay=0.01,               # 重み減衰の強さ\n",
    "    logging_dir='./logs',            # ログ保存フォルダ\n",
    "    eval_accumulation_steps=1,\n",
    "    save_strategy=\"no\",\n",
    "\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b4d526af-589c-4723-8b41-0fe6523858f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from datasets import load_metric\n",
    "\n",
    "f1 = load_metric(\"f1\")\n",
    "acc = load_metric(\"accuracy\")\n",
    "precision = load_metric(\"precision\")\n",
    "recall = load_metric(\"recall\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    params = dict(predictions=predictions, references=labels,)\n",
    "    return { \n",
    "        **acc.compute(**params),\n",
    "        **f1.compute(**params, average=\"weighted\"),\n",
    "        **precision.compute(**params, average=\"weighted\"),\n",
    "        **recall.compute(**params, average=\"weighted\"),\n",
    "       }"
   ]
  },
  {
   "cell_type": "raw",
   "id": "e3546526-05ba-4bac-9950-2a1afa80a8bc",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "source": [
    "model.from_pretrained(\"./results2/checkpoint-10000/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4c5a365f-be20-4f6a-a6d6-853e53c62bde",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer\n",
    "# Trainerの準備\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset['train'],     # 訓練データセット\n",
    "    eval_dataset=tokenized_dataset['test'],        # 評価データセット\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bb82b748-b95d-458b-a32a-b593b78dbdba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running training *****\n",
      "  Num examples = 5525\n",
      "  Num Epochs = 10\n",
      "  Instantaneous batch size per device = 4\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 4\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 13820\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='13820' max='13820' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [13820/13820 1:35:31, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>1.455300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.455800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.466300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.378600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.288900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.234200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>0.186000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.170200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>0.143500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>0.129000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5500</td>\n",
       "      <td>0.132400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>0.072000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6500</td>\n",
       "      <td>0.085300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>0.043300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7500</td>\n",
       "      <td>0.050500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>0.032000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8500</td>\n",
       "      <td>0.025200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9000</td>\n",
       "      <td>0.023400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9500</td>\n",
       "      <td>0.022900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>0.019800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10500</td>\n",
       "      <td>0.010500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11000</td>\n",
       "      <td>0.015600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11500</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12000</td>\n",
       "      <td>0.002100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12500</td>\n",
       "      <td>0.010000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13000</td>\n",
       "      <td>0.003700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13500</td>\n",
       "      <td>0.004000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=13820, training_loss=0.16138851144883443, metrics={'train_runtime': 5732.7766, 'train_samples_per_second': 9.638, 'train_steps_per_second': 2.411, 'total_flos': 1.4537799454464e+16, 'train_loss': 0.16138851144883443, 'epoch': 10.0})"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "14390ba0-cbd6-4501-ac6a-dc9068c9b819",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 1842\n",
      "  Batch size = 4\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='461' max='461' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [461/461 00:41]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.5032410025596619,\n",
       " 'eval_accuracy': 0.9462540716612378,\n",
       " 'eval_f1': 0.9461223435888861,\n",
       " 'eval_precision': 0.9470629203965109,\n",
       " 'eval_recall': 0.9462540716612378,\n",
       " 'eval_runtime': 42.1204,\n",
       " 'eval_samples_per_second': 43.732,\n",
       " 'eval_steps_per_second': 10.945,\n",
       " 'epoch': 10.0}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f90332bf-346c-4dc9-ad40-181a55d4410a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./results2\n",
      "Configuration saved in ./results2/config.json\n",
      "Model weights saved in ./results2/pytorch_model.bin\n"
     ]
    }
   ],
   "source": [
    "trainer.save_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "50e4af62-d674-4c59-b2fa-873869e18bb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_state()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbd46513-6b16-4a17-a958-34330476a342",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
